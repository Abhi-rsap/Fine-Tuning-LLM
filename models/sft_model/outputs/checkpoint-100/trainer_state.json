{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.00628742985586067,
  "eval_steps": 500,
  "global_step": 100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 6.28742985586067e-05,
      "grad_norm": 0.9717520475387573,
      "learning_rate": 0.0,
      "loss": 2.4757,
      "step": 1
    },
    {
      "epoch": 0.0001257485971172134,
      "grad_norm": 0.5979200005531311,
      "learning_rate": 4e-05,
      "loss": 1.7586,
      "step": 2
    },
    {
      "epoch": 0.0001886228956758201,
      "grad_norm": 0.5924529433250427,
      "learning_rate": 8e-05,
      "loss": 1.8375,
      "step": 3
    },
    {
      "epoch": 0.0002514971942344268,
      "grad_norm": 0.6510016322135925,
      "learning_rate": 0.00012,
      "loss": 1.5614,
      "step": 4
    },
    {
      "epoch": 0.0003143714927930335,
      "grad_norm": 0.3231235444545746,
      "learning_rate": 0.00016,
      "loss": 1.1585,
      "step": 5
    },
    {
      "epoch": 0.0003772457913516402,
      "grad_norm": 0.43578824400901794,
      "learning_rate": 0.0002,
      "loss": 1.3675,
      "step": 6
    },
    {
      "epoch": 0.00044012008991024695,
      "grad_norm": 0.4257180392742157,
      "learning_rate": 0.00019789473684210526,
      "loss": 1.6066,
      "step": 7
    },
    {
      "epoch": 0.0005029943884688536,
      "grad_norm": 0.5103814601898193,
      "learning_rate": 0.00019578947368421054,
      "loss": 1.4575,
      "step": 8
    },
    {
      "epoch": 0.0005658686870274604,
      "grad_norm": 0.40405258536338806,
      "learning_rate": 0.0001936842105263158,
      "loss": 1.6557,
      "step": 9
    },
    {
      "epoch": 0.000628742985586067,
      "grad_norm": 0.3619832396507263,
      "learning_rate": 0.00019157894736842104,
      "loss": 1.3417,
      "step": 10
    },
    {
      "epoch": 0.0006916172841446738,
      "grad_norm": 0.5299883484840393,
      "learning_rate": 0.00018947368421052632,
      "loss": 2.432,
      "step": 11
    },
    {
      "epoch": 0.0007544915827032804,
      "grad_norm": 0.434203177690506,
      "learning_rate": 0.0001873684210526316,
      "loss": 1.5743,
      "step": 12
    },
    {
      "epoch": 0.0008173658812618872,
      "grad_norm": 0.2668797969818115,
      "learning_rate": 0.00018526315789473685,
      "loss": 1.1059,
      "step": 13
    },
    {
      "epoch": 0.0008802401798204939,
      "grad_norm": 0.2987941801548004,
      "learning_rate": 0.0001831578947368421,
      "loss": 1.2492,
      "step": 14
    },
    {
      "epoch": 0.0009431144783791005,
      "grad_norm": 0.3524884879589081,
      "learning_rate": 0.00018105263157894739,
      "loss": 1.4413,
      "step": 15
    },
    {
      "epoch": 0.0010059887769377073,
      "grad_norm": 0.28549161553382874,
      "learning_rate": 0.00017894736842105264,
      "loss": 1.4997,
      "step": 16
    },
    {
      "epoch": 0.001068863075496314,
      "grad_norm": 0.31375184655189514,
      "learning_rate": 0.0001768421052631579,
      "loss": 1.9948,
      "step": 17
    },
    {
      "epoch": 0.0011317373740549208,
      "grad_norm": 0.7369820475578308,
      "learning_rate": 0.00017473684210526317,
      "loss": 2.0521,
      "step": 18
    },
    {
      "epoch": 0.0011946116726135273,
      "grad_norm": 0.24901747703552246,
      "learning_rate": 0.00017263157894736842,
      "loss": 1.0095,
      "step": 19
    },
    {
      "epoch": 0.001257485971172134,
      "grad_norm": 0.27058178186416626,
      "learning_rate": 0.0001705263157894737,
      "loss": 1.6594,
      "step": 20
    },
    {
      "epoch": 0.0013203602697307408,
      "grad_norm": 0.2829138934612274,
      "learning_rate": 0.00016842105263157895,
      "loss": 1.2351,
      "step": 21
    },
    {
      "epoch": 0.0013832345682893476,
      "grad_norm": 0.3347832262516022,
      "learning_rate": 0.00016631578947368423,
      "loss": 1.1168,
      "step": 22
    },
    {
      "epoch": 0.0014461088668479543,
      "grad_norm": 0.22956059873104095,
      "learning_rate": 0.00016421052631578948,
      "loss": 1.2162,
      "step": 23
    },
    {
      "epoch": 0.0015089831654065608,
      "grad_norm": 0.3046724796295166,
      "learning_rate": 0.00016210526315789473,
      "loss": 1.759,
      "step": 24
    },
    {
      "epoch": 0.0015718574639651676,
      "grad_norm": 0.354905903339386,
      "learning_rate": 0.00016,
      "loss": 1.326,
      "step": 25
    },
    {
      "epoch": 0.0016347317625237743,
      "grad_norm": 0.3095351457595825,
      "learning_rate": 0.00015789473684210527,
      "loss": 1.2154,
      "step": 26
    },
    {
      "epoch": 0.001697606061082381,
      "grad_norm": 0.33731287717819214,
      "learning_rate": 0.00015578947368421052,
      "loss": 2.1984,
      "step": 27
    },
    {
      "epoch": 0.0017604803596409878,
      "grad_norm": 0.3562106788158417,
      "learning_rate": 0.0001536842105263158,
      "loss": 1.4481,
      "step": 28
    },
    {
      "epoch": 0.0018233546581995946,
      "grad_norm": 0.2762065529823303,
      "learning_rate": 0.00015157894736842108,
      "loss": 1.5937,
      "step": 29
    },
    {
      "epoch": 0.001886228956758201,
      "grad_norm": 0.2120126485824585,
      "learning_rate": 0.00014947368421052633,
      "loss": 1.236,
      "step": 30
    },
    {
      "epoch": 0.0019491032553168078,
      "grad_norm": 0.31021639704704285,
      "learning_rate": 0.00014736842105263158,
      "loss": 1.4441,
      "step": 31
    },
    {
      "epoch": 0.0020119775538754146,
      "grad_norm": 0.27703529596328735,
      "learning_rate": 0.00014526315789473686,
      "loss": 1.4271,
      "step": 32
    },
    {
      "epoch": 0.0020748518524340213,
      "grad_norm": 0.3387010097503662,
      "learning_rate": 0.0001431578947368421,
      "loss": 1.562,
      "step": 33
    },
    {
      "epoch": 0.002137726150992628,
      "grad_norm": 0.28912681341171265,
      "learning_rate": 0.00014105263157894736,
      "loss": 1.5832,
      "step": 34
    },
    {
      "epoch": 0.002200600449551235,
      "grad_norm": 0.2878691554069519,
      "learning_rate": 0.00013894736842105264,
      "loss": 1.2051,
      "step": 35
    },
    {
      "epoch": 0.0022634747481098416,
      "grad_norm": 0.2972211241722107,
      "learning_rate": 0.0001368421052631579,
      "loss": 1.3523,
      "step": 36
    },
    {
      "epoch": 0.0023263490466684483,
      "grad_norm": 0.3261043131351471,
      "learning_rate": 0.00013473684210526317,
      "loss": 1.3339,
      "step": 37
    },
    {
      "epoch": 0.0023892233452270546,
      "grad_norm": 0.633409857749939,
      "learning_rate": 0.00013263157894736842,
      "loss": 1.3943,
      "step": 38
    },
    {
      "epoch": 0.0024520976437856614,
      "grad_norm": 0.2602919340133667,
      "learning_rate": 0.0001305263157894737,
      "loss": 1.2476,
      "step": 39
    },
    {
      "epoch": 0.002514971942344268,
      "grad_norm": 0.30473071336746216,
      "learning_rate": 0.00012842105263157895,
      "loss": 1.1201,
      "step": 40
    },
    {
      "epoch": 0.002577846240902875,
      "grad_norm": 0.3348842263221741,
      "learning_rate": 0.0001263157894736842,
      "loss": 1.2757,
      "step": 41
    },
    {
      "epoch": 0.0026407205394614816,
      "grad_norm": 0.2912239134311676,
      "learning_rate": 0.00012421052631578949,
      "loss": 1.1556,
      "step": 42
    },
    {
      "epoch": 0.0027035948380200884,
      "grad_norm": 0.30516719818115234,
      "learning_rate": 0.00012210526315789474,
      "loss": 1.8085,
      "step": 43
    },
    {
      "epoch": 0.002766469136578695,
      "grad_norm": 0.2756688594818115,
      "learning_rate": 0.00012,
      "loss": 1.3512,
      "step": 44
    },
    {
      "epoch": 0.002829343435137302,
      "grad_norm": 0.26639482378959656,
      "learning_rate": 0.00011789473684210525,
      "loss": 1.2034,
      "step": 45
    },
    {
      "epoch": 0.0028922177336959086,
      "grad_norm": 0.303223580121994,
      "learning_rate": 0.00011578947368421053,
      "loss": 1.285,
      "step": 46
    },
    {
      "epoch": 0.0029550920322545153,
      "grad_norm": 0.31701886653900146,
      "learning_rate": 0.0001136842105263158,
      "loss": 1.458,
      "step": 47
    },
    {
      "epoch": 0.0030179663308131217,
      "grad_norm": 0.2821817100048065,
      "learning_rate": 0.00011157894736842105,
      "loss": 1.6081,
      "step": 48
    },
    {
      "epoch": 0.0030808406293717284,
      "grad_norm": 0.26351141929626465,
      "learning_rate": 0.00010947368421052633,
      "loss": 2.0329,
      "step": 49
    },
    {
      "epoch": 0.003143714927930335,
      "grad_norm": 0.3276241719722748,
      "learning_rate": 0.00010736842105263158,
      "loss": 1.2705,
      "step": 50
    },
    {
      "epoch": 0.003206589226488942,
      "grad_norm": 0.42223137617111206,
      "learning_rate": 0.00010526315789473685,
      "loss": 2.1513,
      "step": 51
    },
    {
      "epoch": 0.0032694635250475486,
      "grad_norm": 0.27898186445236206,
      "learning_rate": 0.00010315789473684211,
      "loss": 1.2402,
      "step": 52
    },
    {
      "epoch": 0.0033323378236061554,
      "grad_norm": 0.45399343967437744,
      "learning_rate": 0.00010105263157894738,
      "loss": 1.5197,
      "step": 53
    },
    {
      "epoch": 0.003395212122164762,
      "grad_norm": 0.31787756085395813,
      "learning_rate": 9.894736842105263e-05,
      "loss": 1.3163,
      "step": 54
    },
    {
      "epoch": 0.003458086420723369,
      "grad_norm": 0.30565935373306274,
      "learning_rate": 9.68421052631579e-05,
      "loss": 1.0141,
      "step": 55
    },
    {
      "epoch": 0.0035209607192819756,
      "grad_norm": 0.265380859375,
      "learning_rate": 9.473684210526316e-05,
      "loss": 1.1637,
      "step": 56
    },
    {
      "epoch": 0.0035838350178405824,
      "grad_norm": 0.36173325777053833,
      "learning_rate": 9.263157894736843e-05,
      "loss": 1.939,
      "step": 57
    },
    {
      "epoch": 0.003646709316399189,
      "grad_norm": 0.5484178066253662,
      "learning_rate": 9.052631578947369e-05,
      "loss": 1.7378,
      "step": 58
    },
    {
      "epoch": 0.0037095836149577954,
      "grad_norm": 0.3810933530330658,
      "learning_rate": 8.842105263157894e-05,
      "loss": 2.185,
      "step": 59
    },
    {
      "epoch": 0.003772457913516402,
      "grad_norm": 0.2541467845439911,
      "learning_rate": 8.631578947368421e-05,
      "loss": 1.3032,
      "step": 60
    },
    {
      "epoch": 0.003835332212075009,
      "grad_norm": 0.42922788858413696,
      "learning_rate": 8.421052631578948e-05,
      "loss": 1.2397,
      "step": 61
    },
    {
      "epoch": 0.0038982065106336157,
      "grad_norm": 0.4108724892139435,
      "learning_rate": 8.210526315789474e-05,
      "loss": 1.3834,
      "step": 62
    },
    {
      "epoch": 0.003961080809192222,
      "grad_norm": 0.5056754946708679,
      "learning_rate": 8e-05,
      "loss": 1.6392,
      "step": 63
    },
    {
      "epoch": 0.004023955107750829,
      "grad_norm": 0.4601009786128998,
      "learning_rate": 7.789473684210526e-05,
      "loss": 2.0762,
      "step": 64
    },
    {
      "epoch": 0.004086829406309436,
      "grad_norm": 0.547485888004303,
      "learning_rate": 7.578947368421054e-05,
      "loss": 1.8357,
      "step": 65
    },
    {
      "epoch": 0.004149703704868043,
      "grad_norm": 0.3686494529247284,
      "learning_rate": 7.368421052631579e-05,
      "loss": 0.807,
      "step": 66
    },
    {
      "epoch": 0.004212578003426649,
      "grad_norm": 0.36658042669296265,
      "learning_rate": 7.157894736842105e-05,
      "loss": 1.3066,
      "step": 67
    },
    {
      "epoch": 0.004275452301985256,
      "grad_norm": 0.3370380699634552,
      "learning_rate": 6.947368421052632e-05,
      "loss": 0.9842,
      "step": 68
    },
    {
      "epoch": 0.004338326600543863,
      "grad_norm": 0.4204678535461426,
      "learning_rate": 6.736842105263159e-05,
      "loss": 2.0016,
      "step": 69
    },
    {
      "epoch": 0.00440120089910247,
      "grad_norm": 0.4740913212299347,
      "learning_rate": 6.526315789473685e-05,
      "loss": 1.1986,
      "step": 70
    },
    {
      "epoch": 0.004464075197661076,
      "grad_norm": 0.2950415313243866,
      "learning_rate": 6.31578947368421e-05,
      "loss": 1.4924,
      "step": 71
    },
    {
      "epoch": 0.004526949496219683,
      "grad_norm": 0.652204155921936,
      "learning_rate": 6.105263157894737e-05,
      "loss": 1.3724,
      "step": 72
    },
    {
      "epoch": 0.00458982379477829,
      "grad_norm": 0.38035911321640015,
      "learning_rate": 5.894736842105263e-05,
      "loss": 1.9448,
      "step": 73
    },
    {
      "epoch": 0.004652698093336897,
      "grad_norm": 0.4956267178058624,
      "learning_rate": 5.68421052631579e-05,
      "loss": 1.6898,
      "step": 74
    },
    {
      "epoch": 0.0047155723918955025,
      "grad_norm": 0.6242780089378357,
      "learning_rate": 5.4736842105263165e-05,
      "loss": 1.5154,
      "step": 75
    },
    {
      "epoch": 0.004778446690454109,
      "grad_norm": 0.37642380595207214,
      "learning_rate": 5.2631578947368424e-05,
      "loss": 1.0131,
      "step": 76
    },
    {
      "epoch": 0.004841320989012716,
      "grad_norm": 0.3036065995693207,
      "learning_rate": 5.052631578947369e-05,
      "loss": 1.4348,
      "step": 77
    },
    {
      "epoch": 0.004904195287571323,
      "grad_norm": 0.40423324704170227,
      "learning_rate": 4.842105263157895e-05,
      "loss": 1.4779,
      "step": 78
    },
    {
      "epoch": 0.0049670695861299295,
      "grad_norm": 0.42193737626075745,
      "learning_rate": 4.6315789473684214e-05,
      "loss": 1.4356,
      "step": 79
    },
    {
      "epoch": 0.005029943884688536,
      "grad_norm": 0.3145669102668762,
      "learning_rate": 4.421052631578947e-05,
      "loss": 1.7514,
      "step": 80
    },
    {
      "epoch": 0.005092818183247143,
      "grad_norm": 0.35116857290267944,
      "learning_rate": 4.210526315789474e-05,
      "loss": 1.3074,
      "step": 81
    },
    {
      "epoch": 0.00515569248180575,
      "grad_norm": 0.28581517934799194,
      "learning_rate": 4e-05,
      "loss": 1.0598,
      "step": 82
    },
    {
      "epoch": 0.0052185667803643565,
      "grad_norm": 0.4222072660923004,
      "learning_rate": 3.789473684210527e-05,
      "loss": 1.5244,
      "step": 83
    },
    {
      "epoch": 0.005281441078922963,
      "grad_norm": 0.3449529707431793,
      "learning_rate": 3.578947368421053e-05,
      "loss": 1.063,
      "step": 84
    },
    {
      "epoch": 0.00534431537748157,
      "grad_norm": 0.3756192922592163,
      "learning_rate": 3.368421052631579e-05,
      "loss": 1.4564,
      "step": 85
    },
    {
      "epoch": 0.005407189676040177,
      "grad_norm": 0.5120066404342651,
      "learning_rate": 3.157894736842105e-05,
      "loss": 0.9142,
      "step": 86
    },
    {
      "epoch": 0.0054700639745987835,
      "grad_norm": 0.3662187159061432,
      "learning_rate": 2.9473684210526314e-05,
      "loss": 1.1284,
      "step": 87
    },
    {
      "epoch": 0.00553293827315739,
      "grad_norm": 0.432017058134079,
      "learning_rate": 2.7368421052631583e-05,
      "loss": 1.2958,
      "step": 88
    },
    {
      "epoch": 0.005595812571715997,
      "grad_norm": 0.2894713580608368,
      "learning_rate": 2.5263157894736845e-05,
      "loss": 1.2532,
      "step": 89
    },
    {
      "epoch": 0.005658686870274604,
      "grad_norm": 0.3900812268257141,
      "learning_rate": 2.3157894736842107e-05,
      "loss": 1.4493,
      "step": 90
    },
    {
      "epoch": 0.0057215611688332104,
      "grad_norm": 0.47248393297195435,
      "learning_rate": 2.105263157894737e-05,
      "loss": 1.5983,
      "step": 91
    },
    {
      "epoch": 0.005784435467391817,
      "grad_norm": 0.41463717818260193,
      "learning_rate": 1.8947368421052634e-05,
      "loss": 1.9463,
      "step": 92
    },
    {
      "epoch": 0.005847309765950424,
      "grad_norm": 0.32716745138168335,
      "learning_rate": 1.6842105263157896e-05,
      "loss": 1.1924,
      "step": 93
    },
    {
      "epoch": 0.005910184064509031,
      "grad_norm": 0.2799433767795563,
      "learning_rate": 1.4736842105263157e-05,
      "loss": 1.2659,
      "step": 94
    },
    {
      "epoch": 0.005973058363067637,
      "grad_norm": 0.4479924440383911,
      "learning_rate": 1.2631578947368422e-05,
      "loss": 1.4825,
      "step": 95
    },
    {
      "epoch": 0.006035932661626243,
      "grad_norm": 0.3391727805137634,
      "learning_rate": 1.0526315789473684e-05,
      "loss": 1.6377,
      "step": 96
    },
    {
      "epoch": 0.00609880696018485,
      "grad_norm": 0.3394221067428589,
      "learning_rate": 8.421052631578948e-06,
      "loss": 1.099,
      "step": 97
    },
    {
      "epoch": 0.006161681258743457,
      "grad_norm": 0.2234630584716797,
      "learning_rate": 6.315789473684211e-06,
      "loss": 0.8292,
      "step": 98
    },
    {
      "epoch": 0.0062245555573020635,
      "grad_norm": 0.2812862992286682,
      "learning_rate": 4.210526315789474e-06,
      "loss": 1.3778,
      "step": 99
    },
    {
      "epoch": 0.00628742985586067,
      "grad_norm": 0.37536337971687317,
      "learning_rate": 2.105263157894737e-06,
      "loss": 1.1921,
      "step": 100
    }
  ],
  "logging_steps": 1,
  "max_steps": 100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3186178937776128.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
